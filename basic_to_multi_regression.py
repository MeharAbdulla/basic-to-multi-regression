# -*- coding: utf-8 -*-
"""basic-to-multi-regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cnXdc98f8MHvngqStqMemwfzdgQX8cD2

# Linear Regression with One Variable

Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. The file **ex1data1.txt** (available under week 2's assignment material) contains the dataset for our linear regression exercise. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss.
"""

# !pip install matplotlib

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import os

data = pd.read_csv('ex1data1.txt', header = None) #read from dataset
X = data.iloc[:,0] # read first column
y = data.iloc[:,1] # read second column
m = len(y) # number of training example
data.head() # view first few rows of the data

"""### Plot Dataset using scatter to visualize the pattern in it"""

# Plot Data
plt.scatter(X, y)
plt.xlabel('Population of City in 10,000s')
plt.ylabel('Profit in $10,000s')
plt.show()

"""### Add Intercept and Parameters initilization

we add another dimension to our data to accommodate the intercept term (the reason for doing this is explained in the videos). We also initialize the initial parameters theta to 0 and the learning rate alpha to 0.01.
"""

#### DONT CHANGE THIS CODE ####
X = np.array(X)
y = np.array(y)
X = X[:,np.newaxis]
y = y[:,np.newaxis]
# Hyperparameters
iterations = 1500
alpha = 0.01

## Write the code ##
# Initlize Parameters () to ZERO
theta = np.zeros((2, 1))
# Initilize one to be multiplied with intercept

# Stack ones withe actual data set as it will look like following output.

X

print(theta)

"""### COST FUNCTION ####
Implement the MSE cost function for above mentioned dataset. The formula is as followed.
![Mean_Squared_Error](res/MSE.png "MSE")

"""

### WRITE CODE HERE ###
# NOTE:- You may use vectorized implemention of cost function using numpy library.
# Hint:- np.dot function will help you to implement vectorized dot product of theta and input features.

def computeCost(X, y, theta):
    size = len(y)
    X = np.hstack((np.ones((X.shape[0], 1)), X))

    n = np.dot(X, theta)
    errors = n - y.reshape(-1, 1)
    cost = (1 / (2 * size)) * np.sum(errors ** 2)

    return cost

# Initial Cost
print(y.shape)
print(X.shape)
J = computeCost(X, y, theta)

print(J)

"""### Finding the optimal parameters using Gradient Descent¶
The Paritial Derivative of the Cost function is:
![derivatives](res/derivatives.png "MSE")
"""

### WRITE CODE HERE ###
# NOTE:- According to passed parameters .
# Hint:- Don't Directly update the actual theta but use temp variable during update.
import numpy as np

def gradientDescent(X, y, theta, alpha, iterations):
    m = len(y)

    X = np.hstack((np.ones((m, 1)), X))
    y = y.reshape(-1, 1)

    for _ in range(iterations):
        n = np.dot(X, theta)
        errors = n - y
        gradient = (1 / m) * np.dot(X.T, errors)
        theta = theta - alpha * gradient

    return theta

theta = gradientDescent(X, y, theta, alpha, iterations)
print(theta)

"""Expected theta values [-3.6303, 1.1664]


"""

# Calculate the Cost after converging of thetas for number of iterations.
J = computeCost(X, y, theta)
print(J)

"""### Plot showing the best fit line¶

"""

## Expected Output plot....##
X = np.hstack((np.ones((X.shape[0], 1)), X))
plt.scatter(X[:,1], y, label='Training data')  # Plot the actual points
plt.plot(X[:,1], np.dot(X, theta), color='red', label='Linear regression')  # Plot the prediction line
plt.xlabel('Population of City in 10,000s')
plt.ylabel('Profit in $10,000s')
plt.legend()
plt.savefig('graph.png')
plt.show()

"""## Linear Regression with Multiple Predictors (Variable)

**Problem context:** Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices. Your job is to predict housing prices based on other variables.


The first column is the **size of the house (in square feet)**, the second column is the **number of bedrooms**, and the third column is the **price** of the house.
"""

import numpy as np
import pandas as pd

# Load data
data = pd.read_csv('ex1data2.txt', sep=',', header=None)
X_original = data.iloc[:, 0:2].values  # (m x 2)
y = data.iloc[:, 2].values.reshape(-1, 1)  # (m x 1)

m = len(y)
X_original

"""As can be seen above we are dealing with more than one independent variables here (but the concepts you have learnt in the previous section applies here as well)

## Feature Scaling and Normalization

By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.


Feature Scaling can be performed with following steps.
1. Subtract the mean value of each feature from the dataset.
1. After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”

### Apply the Z-Score Scaling the features. And use following Z-Score Scaling foru
![scaling](res/scaling.png "MSE")
"""

# Feature normalization
mu = np.mean(X_original, axis=0)
sigma = np.std(X_original, axis=0)
X_norm = (X_original - mu) / sigma
X_norm

### WRITE CODE HERE ###
# In above features add a column of ones in dataset for intercept term.
# Add bias column (1s) after normalization

X = np.hstack((np.ones((m, 1)), X_norm))

theta = np.zeros((X.shape[1], 1))  # (3 x 1)

# Hyperparamters. Don't change there for submission
alpha = 0.01
num_iters = 400

## WRITE CODE HERE ##
# Initialize Thetas to ZERO for above dataset.
theta

## WRITE CODE HERE ###
# Complete cost function for more than one input features.
def computeCostMulti(X, y, theta):
    m = len(y)
    predictions = np.dot(X, theta)
    errors = predictions - y
    cost = (1 / (2 * m)) * np.sum(errors ** 2)
    return cost


J = computeCostMulti(X, y, theta)
print(J)
# Expected Output.

def gradientDescentMulti(X, y, theta, alpha, iterations):
    m = len(y)
    cost_history = []

    for _ in range(iterations):
        pred = np.dot(X, theta)
        errors = pred - y
        gradient = (1 / m) * np.dot(X.T, errors)
        theta = theta - alpha * gradient
        cost = computeCostMulti(X, y, theta)
        cost_history.append(cost)

    return theta, cost_history

theta, cost_history = gradientDescentMulti(X, y, theta, alpha, num_iters)
print("Theta after gradient descent:\n", theta)

initial_cost = computeCostMulti(X, y, theta)
print("Initial Cost:", initial_cost)

